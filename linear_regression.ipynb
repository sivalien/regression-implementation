{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначения:\n",
    "- $\\mathbb{X} \\in \\mathbb{R}^{n \\times m}$ - матрица признаков\n",
    "- $\\mathbb{Y} \\in \\mathbb{R}^n$ - целевая переменная\n",
    "- $x \\in \\mathbb{R}^{m}$ - один объект из выборки\n",
    "- $\\omega = (\\omega_1,...,\\omega_m)$ - вектор весов (параметров) модели\n",
    "- $Q(f, \\mathbb{Y})$ - функционал ошибок  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся градиентным спуском для поиска минимума функционала ошибок, на каждой итерации изменяем веса по следующей формуле: \n",
    "   \n",
    "$\\omega^{(k)} = \\omega^{(k - 1)} - \\eta \\nabla_{\\omega}Q(\\omega^{(k-1)})$, где $\\eta$ - learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейной регресии функции потерь: $Q = MSE = \\sum\\limits_{i}(y_i-x_i^T \\omega)^2 = ||\\mathbb{Y} - \\mathbb{X}\\omega||_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём шаг градиентного спуска\n",
    "\n",
    "$||\\mathbb{Y} - \\mathbb{X}\\omega||_2^2 = <\\mathbb{Y} - \\mathbb{X}\\omega, \\mathbb{Y} - \\mathbb{X}\\omega>$\n",
    "\n",
    "Дифференциал для векторной функции: $df(x)\\big|_{x_0} = <\\nabla f(x), x_0>$\n",
    "\n",
    "Формула дифференциала скалярного произведения: $d(<u, v>)\\big|_{\\omega} = <d(u)|_{\\omega}, v> + <u, d(v)|_{\\omega}>$\n",
    "\n",
    "$dQ\\big|_{\\omega_{*}} = 2<d(\\mathbb{Y} - \\mathbb{X}\\omega)\\big|_{\\omega_{*}}, \\mathbb{Y} - \\mathbb{X}\\omega>$\n",
    "\n",
    "$dQ\\big|_{\\omega_{*}} = -2<\\mathbb{Y} - \\mathbb{X}\\omega, \\mathbb{X}\\omega_{*}> = -2\\mathbb{X}^T<\\mathbb{Y} - \\mathbb{X}\\omega, \\omega_{*}>$\n",
    "\n",
    "$\\nabla_{\\omega}Q = -2\\mathbb{X}^T(\\mathbb{Y} - \\mathbb{X}\\omega) = 2\\mathbb{X}^T(\\mathbb{X}\\omega - \\mathbb{Y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class LinReg(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, batch_size=25, num_steps=350, lr=1e-2):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        w = np.random.randn(X.shape[1])[:, None]\n",
    "        n_objects = len(X)\n",
    "\n",
    "        for i in range(self.num_steps):\n",
    "            sample_indices = np.random.randint(0, n_objects, size=self.batch_size)\n",
    "            w -= 2 * self.lr * np.dot(X[sample_indices].T, np.dot(X[sample_indices], w) - Y[sample_indices]) / self.batch_size\n",
    "\n",
    "        self.w = w\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X@self.w"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
